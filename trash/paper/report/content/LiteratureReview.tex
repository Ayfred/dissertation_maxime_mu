\chapter{Literature Review}


\section{Why is Tabular Data So Important ?}
\label{sec:whytabularsoimportant}

\subsection{Tabular is Used Everywhere}

Tabular data has been increasingly used for data formatting in the context of machine learning (ML), computer vision and data analysis (\cite{Lautrup2024}) as it allows for easy data manipulation and organisation; enabling computers to understand and correlate linked features. The platform Google Dataset Search Platform is one of the popular examples of tabular data references \cite{Borisov2023}. According to studies, they are widely used in several domains such as healthcare, finance, psychology, and anomaly detection. (\cite{Johnson2016, Ulmer2020, Urban2021, Guo2017, AE2022}) 

\subsection{Key Attributes of Tabular Data}

Tabular data is fundamental in disciplines such as healthcare or computer science because it stems from several key attributes that make them almost the only one that gives such numerous advantages of data manipulation, organisation and analyses. 


%%%%%%%%%%%%%%%% SIMPLICITY %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{0.5cm}
First, its importance mainly comes from its simplicity. In fact, tabular data gives information a simple and meaningful form transforming the bulk of data into a simpler form. It is generally represented as a table in two dimensions and it is possible to give our interpretation of the rows and columns. For example, in the IMDB movie dataset that references popular movies \cite{IMDbNonCommercial2024}, the columns represent the movie's characteristic names such as the title, date of publication, actors and actresses playing in the movie. In that case, the rows would represent one type of movie, thus another row would represent another movie. This makes the data more structured as each movie is referenced in each row, all the movie titles are categorised under the same column name and the same goes for all the other features. This structuring makes it not only easily readable for us, users but also for computers as everything is well structured, and nowadays many programming languages support reading this type of data format. 

\vspace{0.5cm}
Thus, it is possible to access a specific movie in the tabular data, and one of the examples for searching a movie is the query search using SQL statement. SQL is a powerful programming language that can retrieve data from a SQL statement. Nonetheless, the Python library Pandas offers a wide range of manipulation tools such as direct access to a movie feature showing only the last movies in case the tabular data file is too large. In the case of the healthcare domain, hospitals can sometimes be faced with thousands of patient records which can be quite time-consuming if there are no adapted tools for searching for a specific patient.

%%%%%%%%%%%%%% VERSATILITY %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{0.5cm}
The second is its versatility in data analysis. In fact, tabular data due to its structure can be easily analysed which makes it suitable for statistical analysis and machine learning techniques. This can be advantageous in fields such as economics or social sciences as numerous tools can perform statistical analysis on tabular data such as the R programming language or Python using the statistical analysis libraries. For example, it is possible to represent an evolution of the cost of a certain product over the years or estimate the cost of a wholesale product for a company. In the healthcare domain, analysing patients' data can be proven quite useful as it might help in conducting research and studies that help prevent future diseases or upcoming diseases for a patient. In fact, by analysing the patient's disease, it is possible to detect unknown diseases or symptoms that can be the overall treatment for all patients faster. Machine learning techniques work very well with this type of data, it is suitable for making, for example, sales predictions using advanced machine learning models such as decision trees, random forests, and gradient boosting.

%%%%%%%%%%%%%%%%%%% STANDARDIZATION %%%%%%%%%%%%%%%%%%%%%%%
\vspace{0.5cm}
Third, standardization allows the tabular data to have compatibility support across different software and platforms. Standard formats for the tabular data are CSV (Comma-Separated Values) and Excel spreadsheets are the most well-known. The advantage of having the standardisation is that it facilitates data sharing and collaboration among researchers, analysts and organisations. Several converting tools exist, that allow to convert any text format or even Excel spreadsheet format into a CSV format, and vice versa. These tools are either integrated into the software itself, or there are external software that perform conversion.

%%%%%%%%%%%%%%%%%%%%%%%% DBMS %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{0.5cm}
Finally, data management is more efficient when using tabular data. Database management systems (DBMS) use tabular data as they are optimized for storing, querying, and managing large volumes of data efficiently. 










\section{The Rise of Synthetic Data and its Applications}


Synthetic data generation has become popularized due to its potential benefits in solving pressing challenges in various domains like data science, and health care. This section explores the different aspects of synthetic data generation, more specifically the usage, the techniques used, and the potential limitations.

\subsection{Exploring Synthetic Data Generation}

%%%%%%%%%%%%%%%% SECURITY %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Data generation has been proven extremely useful in domains where data privacy and security are paramount. For instance, in the healthcare domain, patients' health records are sensitive and difficult to access in order to perform scientific research on them; which could be very advantageous for the medical domain in diagnosing diseases. 


%%%%%%%%%%%%%%%%%%%%% GANs %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{0.5cm}
The paper \cite{Choi2017} proposes a novel approach leveraging GANs to generate realistic synthetic patient records. In that way, synthetic patient records help avoid exposing real patient information, thus allowing researchers to develop and test new treatments without compromising patient information. 


%%%%%%%%%%%%%% LLM %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{0.5cm}
LLMs have also been leveraged to generate synthetic data; they have particularly been used in the context of natural language processing tasks such as toxicity detection on online discussions. One advantage of the generated data is that it can be used to create vast training datasets, commonly known as data augmentation \cite{Kruschwitz2024}. They are used for downstream improvement of robust machine-learning models (\cite{Radford2018}). They could detect and filter toxic language in online discussions, which improves the overall safety and user experience (\cite{Kruschwitz2024}). In the research paper \cite{Park2024}, the authors explore the potential advantage of using synthetic data for data augmentation to enhance datasets for specific applications. It was shown that synthetic data can increase the diversity and realism of the generated datasets, which then can be utilized to train machine learning models making them more robust. For instance, when a grammatical error is made, the ChatLang-8 framework can mimic the types of errors made by humans and generate the data based on these errors, the data can then be used as training data for improving grammatical error correction models.




\subsection{Challenges in Generating High-Quality Synthetic Data}

%%%%%%%%%%%%%%%%%%%%%% LEAK %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Despite the potential of synthetic data, when it comes to generating it remains a challenging task. In fact, one of the primary concerns is data leak and this can be especially seen in fields like healthcare, where patient data must be preserved and each manipulation must be at the utmost care. Techniques in preserving confidential data exist, two examples of these techniques are differential privacy and federated learning. Differential privacy ensures that synthetic data does not reveal any confidential patient information from the original data. On the other hand, federated learning allows machine learning models to be trained on decentralized data sources without the data ever leaving its source (\cite{Dwork2008}).

%%%%%%%%%%%%%%%%%% INTEGRITY %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{0.5cm}
Maintaining the statistical integrity of synthetic data is also a challenging task. As a matter of fact, the synthetic data must accurately capture the complex relationships present in the original data in order to be used for the same purpose as the original data. Examples of such statistical integrity are the correlation between variables and class representations. If the synthetic data fails to preserve the statistical integrity, any analysis could lead to a misleading conclusion (\cite{Snoke2018}).

%%%%%%%%%%%%%%%%%%%%%%%% BIAS %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{0.5cm}
Another challenge is the risk of introducing biases during the data synthesis process. This can adversely affect the performance of the machine learning models trained on those data as it could lead to inaccurate predictions. For instance, if a demographic group is underrepresented in the original data, but the same demographic group is over-represented in the synthetic data; models trained on the synthetic data will over-represent the under-represented demographic group which leads to biased outcomes (\cite{Mehrabi2019}). 



\section{Non-LLM Based Methods for Synthetic Data Generation}

% synthpop: Bespoke Creation of Synthetic Data in R
% link https://www.jstatsoft.org/article/view/v074i11

% Synthetic Data Generation: A Comparative Study
% link https://dl.acm.org/doi/abs/10.1145/3548785.3548793

% Assessing, visualizing and improving the utility of synthetic data
% link https://arxiv.org/abs/2109.12717

% Providing bespoke synthetic data for the UK Longitudinal Studies and other sensitive data with the synthpop package for R1
% https://content.iospress.com/articles/statistical-journal-of-the-iaos/sji150153

% Simulation of synthetic complex data: The R package simPop
% https://digitalcollection.zhaw.ch/handle/11475/5698


Before the advent of LLMs, non-LLM-based approaches to generating synthetic data were widely utilized. The study \cite{Kruschwitz2024} discusses that several non-LLM-based methods such as mathematical generation, synonym replacement, and oversampling techniques can be considered. Mathematical generation employs mathematical operations to generate new data based on existing data. Synonym replacement simply suggests that words in a text are replaced with their synonyms to create variations in the original text. Oversampling techniques aim to rebalance the training data distribution by amplifying the volume of under-represented data.




\subsection{Review of Popular Non-LLM Based Models}

Numerous non-LLM-based models exist in the field of synthetic data generation. These include:

\begin{itemize}
    \item Generative Adversarial Networks (GANs): GANs are machine learning frameworks created by Goodfellow et al. (\cite{goodfellow2014generative}). They are composed of two neural networks, more specifically a generator and a discriminator, in which they compete with each other. The generator creates the synthetic data, while the discriminator compares the generated synthetic data with the original data. The two components complete each other to create increasingly realistic synthetic data.

    \item VAEs, introduced by Kingma and Welling in \cite{Kingma2014} are neural networks used to generate synthetic data. They first encode the input data into a low-dimension latent space and then decode it back to the original dimension by adding some random noises to generate synthetic data points.
\end{itemize}


\subsection{Their Limitations}

Despite its popularity and effectiveness in generating synthetic data, their utilisation has several limitations. These include:

\begin{itemize}
    \item Quality: Despite being very performant, GANs and VAEs sometimes fail to fully grasp the complexity of the original data, which can lead to a lack of nuances in the synthetic data.
    \item Overfitting issues: Non-LLM-based approach can suffer from overfitting, this is especially seen when the synthetic data is based on limited or highly specific training data (\cite{Salimans2016}).
\end{itemize}



Ultimately, researchers have developed sophisticated methods to generate high-quality synthetic datasets: Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), and Bayesian networks. Each method has pros and cons; GANs are particularly effective in generative data that closely mimic real data by training two neural networks in a competitive process. VAEs, on the other hand, grasp the statistical structure of the data to generate new and similar data points. Finally, Bayesian networks use probabilistic methods to represent conditional dependencies between variables, which is particularly used to generate data with a similar statistical distribution to the original data (\cite{Davila2024}).








\section{LLMs as realistic data generator}

There are generally two approaches for data generation using LLMs. One is zero-shot learning in which we are asking LLMs to generate data without any sample data in the prompt. The other one is few-shot learning in which we give a sample of the real-world data to help guide the LLMs on what to exactly synthesize. The research paper \cite{Li2023} proposes to analyse the difference between those two settings. Their pipeline aims to generate synthetic data according to the two different learning methods and evaluate the data by passing it to a machine learning model to measure its performance. However, results showed that the model performed generally worse on the data generated using the zero-shot learning compared to the few-shot learning.

\subsection{A Look at the Diverse Landscape of LLMs}

In today's technology, numerous LLMs have surfaced and this became drastically even more important when OpenAI's ChatGPT 3.5 was made available to the public. Since then, other LLMs have started to see the light such as GPT-4 (\cite{OpenAI2023}), Meta's LLaMa-2 and LLaMa-3 (Large Language Model Meta AI) (\cite{touvron2023llama2, touvron2023llama}), Mistral from Mistral AI, Gemma and Gemini from Google (\cite{touvron2024gemini}).


\subsection{How LLMs are Powering Synthetic Data Generation}


The emergence of LLMs opened new horizons for generating data. It can enhance stance detection in online political discussions as the following study suggests (\cite{Wagner2024}). Stance detection is an interesting yet complex task in the sentiment analysis domain that can automatically determine the author's political position (e.g., favour, against, or neutral). In political discussions, it would help in summarising and detecting misinformation. However, this still remains a difficult task as the data available to perform the analysis for stance detection is limited due to privacy reasons. One of the two ways presented in the research article to address this issue is to generate synthetic data by using LLMs, here, the authors opted for the quantized (lightweight version) of Mistral v0.1. A prompt is given to the LLM, and the response generated by the model represents the synthetic data that can significantly augment the existing datasets, thus providing more samples to perform stance detection. Results showed substantial improvements in the field of stance detection highlighting the practical benefits of this approach.

\vspace{0.5cm}
The study \cite{Kruschwitz2024} has used a fined-tuned version of GPT-3, called GPT-3 Curie in the context of toxicity detection from online discussions. 
As a matter of fact, such a study would help prevent hate speech, abusive language, and cyberbullying which has drawn significant attention due to its legal implications under regulations like the EU's Digital Service Act (DSA). 

% ChatLang-8 An LLM-Based Synthetic Data Generation Framework for Grammatical Error Correction

\vspace{0.5cm}
The paper \cite{Park2024} proposes a Framework ChatLang-8 which makes use of the LLM ChatGPT version "gpt3.5-turbo" to demonstrate its ability to generate high-quality synthetic data related to the Grammatical Error Correction (GEC). GEC datasets are used to train and evaluate machine learning models that can automatically correct grammatical errors in text. Results proved the efficiency of using LLMs to generate a more uniform pattern composition compared to the existing GEC datasets, which makes it the ideal candidate for training GEC models.

% Language Models are realistic tabular data generators











% A Deep Learning Based pipeline for the generation of synthetic tabular data









\subsection{Prompt Engineering is an Important Step in Unlocking the Potential of LLMs}

Prompt engineering is a major step before using LLMs, as they would provide elements of structure and request that the model must respect. In case of bad prompt engineering, this could lead to poor results, bad structure of the response, and a response that could potentially not even correspond to the initial intention of the user. 
If carefully designed, the prompt would help guide LLMs towards a satisfactory and relevant response. The study \cite{Kruschwitz2024} in Toxicity Detection performed prompt engineering by including a general structure to the prompt, bias mitigation, and an evaluation of data variability and coherence. 



\section{Evaluating the Quality and Effectiveness of Synthetic Data}

\subsection{Metrics for Evaluating Synthetic Data Quality}

The quality of the data can be evaluated using several metrics. These include:

\begin{itemize}
    \item[1.] \textsc{Statistical similarity} \\
    It measures how closely the synthetic data and the original data match together. Metrics such as mean and standard deviation, and correlation coefficients can be utilized (\cite{Snoke2018}).
    \item[2.] \textsc{Privacy} \\
    It measures how the synthetic data protects an individual's data from the original data. Metrics such as differential privacy can be used (\cite{Dwork2008}).
\end{itemize}

\subsection{Metrics for Evaluating Synthetic Data Effectiveness}

The effectiveness of the data can be evaluated using machine learning and visualization techniques.

\begin{itemize}
    \item[1.] \textsc{Machine learning model performance}\\ 
    Training machine learning models on synthetic data and evaluating them on real data to compare their performance. Metrics such as accuracy, precision, recall and F1 can be used (\cite{Chawla2002}).
    \item[2.] \textsc{Visualization techniques} \\
    Techniques such as t-SNE (t-distributed Stochastic Neighbor Embedding) and PCA (Principal Component Analysis) can be used to visualize the similarity between the synthetic and the original data (\cite{Maaten2008}).
\end{itemize}


% \section{Ethical Considerations in LLM-Generated Synthetic Data}



%Ethical question is a crucial element when talking about LLMs. As a matter of fact, LLMs, in order to generate a satisfactory response, need to analyse the content of the prompt before generating the results. It is not clearly known how companies who possesses those LLMs actually store the data, but 



% \section{The Future of Synthetic Data with LLMs}
