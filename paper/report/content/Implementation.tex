\chapter{Implementation}
\label{ch:implementation}


\section{Configuration File}

The configuration file is designed to facilitate and centralize the modifications of models' parameters. It includes general information about the model parameters as well as the dataset path, discrete columns, number of samples, and the output file path.

More details of the implementation are in the following appendix \ref{app:config_implementation}.

\section{Data Pre-processing}

\section{Data Post-processing}


\vspace{0.5cm}
\section{CTGAN}




%\textbf{Key Features of CTGAN:}

%1. \textbf{Mode-Specific Normalization:} CTGAN employs a technique called mode-specific normalization to handle continuous columns effectively. This technique normalizes the continuous data within each mode (i.e., each category of the categorical columns), making it easier for the generator to learn the data distribution.

%2. \textbf{Conditional Sampling:} By conditioning the generator on the categorical variables, CTGAN ensures that the generated data maintains the correct relationships between categorical and continuous columns. This conditional sampling is critical for generating realistic synthetic data.

%3. \textbf{Training-by-Sampling:} CTGAN uses a training-by-sampling strategy to address the issue of imbalanced data distributions. It samples batches of data during training in a way that ensures all categories are represented adequately, which helps the model learn better.


The synthetic data generator using the model CTGAN is implemented in Python. A class is created, called \texttt{SyntheticDataGeneratorCTGAN}, which handles the initialization, fitting, and generation of synthetic data using the CTGAN model.

\begin{enumerate}
    \item[1.] \textsc{Configuration Reading}\\ 
    The class reads configuration settings from a file.
    
    \item[2.] \textsc{Model Initialization}\\
    CTGAN is initialized with 32 epochs. The number of epochs was determined through trial and error to find the most suitable settings for generating high-quality synthetic data.
    
    \item[3.] \textsc{Model Fitting and Data Generation}\\ 
    The model is fitted to the dataset and synthetic data is generated.

\end{enumerate}


\noindent More details of the implementation and the Python code can be found in the appendix \ref{app:ctganimplementation}.



\vspace{0.5cm}
\section{BeGreat}

To use the BeGreat model, the author of the model has provided the Python library which can be directly imported using the Python pip command.

The synthetic data generator is encapsulated in the class \texttt{SyntheticDataGeneratorBeGreat}. This class manages the initialization, fitting, and generation of synthetic data using the GReaT model.

\begin{enumerate}
    \item[1.] \textsc{Configuration Reading}\\ 
    The class reads configuration settings from a file.
    
    \item[2.] \textsc{Model Initialization}\\
    The model is initialized with the following hyperparameters:
        \begin{enumerate}
            \item llm = distilgpt2 (model name)
            \item batch\_size = 64, determined through trial and error
            \item epochs = 32, determined through trial and error
            \item save\_steps = 400000, default value
            \item n\_samples = size of the dataset
        \end{enumerate}

    \item[3.] \textsc{Model Fitting}\\ 
    The model is fitted to the dataset.
    
    \item[4.] \textsc{Data Generation}\\
    Synthetic data is generated using the fitted model.
\end{enumerate}

\noindent More details of the implementation can be found in the following appendix \ref{app:begreat_implementation}






\vspace{0.5cm}
\section{LLaMa-3}

As for the LLaMa-3 model, the model's weights are downloaded using the shell script given in Meta's Llama3 GitHub web page and the snippet code provided also on the GitHub web page. 

The implementation is encapsulated in a Python script, which manages the configuration reading, data formatting, model initialization, synthetic data generation, and conversion processes.

\begin{enumerate}
    \item[1.] \textsc{Configuration Reading}\\ 
    The class reads configuration settings from a file.
    
    \item[2.] \textsc{Data Formatting}\\
    Tabular patient data is formatted into a textual format using the \texttt{TabularToTextualConverter} class.
    
    \item[3.] \textsc{Model Initialization}\\
    The LLaMa model is initialized with the following hyperparameters:
    \begin{enumerate}
        \item temperature: float = 0.6 (default value)
        \item top\_p: float = 0.9 (default value)
        \item max\_seq\_len: int = 8192 (maximum value), it indicates the number of characters authorized for the text input.
        \item max\_batch\_size: int = 4 (default value)
        \item max\_gen\_len: Optional[int] = None (default value)
    \end{enumerate}
    
    \item \textsc{Synthetic Data Generation}\\
    Textual patient records are generated using the LLaMa model.
        
    \item \textsc{Data Conversion} The generated text is converted back to tabular format using the \texttt{TextualToTabularConverter} class.
\end{enumerate}


More details of the implementation are in the following appendix \ref{app:llama_implementation}

\vspace{0.5cm}
\section{Prompt Engineering}


%Gemma-7b: The Gemma-7b model's weights are downloaded directly from Kaggle using the Command Line Interface. 

%Mistral-7b: Luckily enough, HPC possesses a shared LLMs' weights folder which contains the Mistral-7b model's weights. I just needed to write the code that loads the model using the provided weights.



It is important to consider giving clear instructions on what do we exactly request for the model, this step is known as Prompt Engineering. Each model interprets the same text prompt differently, some models will fail to understand what we are requesting while others may understand it perfectly. 
After engineering the text prompts to get the best response from the LLMs, the following input was provided to all models:

\begin{quote}
    \textit{
        "Generate \textbf{x} additional patient records in the following format and generate new diseases: \\
        Patient \textbf{i}: [Disease: \textbf{disease}, Fever: \textbf{fever}, Cough: \textbf{cough}, Fatigue: \textbf{fatigue}, Difficulty Breathing: \textbf{difficulty\_breathing}, Age: \textbf{age}, Gender: \textbf{gender}, Blood Pressure: \textbf{blood\_pressure}, Cholesterol Level: \textbf{cholesterol\_level}, Outcome Variable: \textbf{outcome}] \\
        Use this current data for reference: \\
        Data: "
    }
\end{quote}


The responses of the LLMs tend to be different from each other. The models were able to follow the instructions in the text prompt with a few exceptions. Some models can give a slight variation of the format. The inconsistency of the response formats is treated in another data processing, called data post-processing.

