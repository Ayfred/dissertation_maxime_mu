\chapter{Literature Review}


\section{Why is Tabular data so important ?}
% \section{The Rise of Synthetic Data and its Applications}

Tabular data is by far the most used data formatting in a context of machine learning (ML) as it allows for easy data manipulation and organisation that permit for computer to easily understand and do correlation between linked features. The platform Google Dataset Search Platform is one of the popular example of listed tabular data that is either in CSV or XLS formats. [Language Models are Realistic Tabular Data] According to studies, they are widely used in several domains such as healthcare, finance, psychology, and anomaly detection. (Johnson et al.,
 2016; Ulmer et al., 2020; Urban \& Gates, 2021; Chandola
 et al., 2009; Guo et al., 2017; A. \& E., 2022) 
The reasons why tabular data is fundamental in disciplines such as healthcare or computer science is that it stems from several key attributes that make them almost the only one that gives such numerous advantages of data manipulation, organisation and analyses. 

First, its importance mainly comes from its simplicity. In fact, it is in a form of a table in two dimensions with rows and columns. It is possible to give our own interpretation of the rows and columns. For example, the IMDB movie dataset that references the popular movies, the columns represent the movie characteristic names such as the title, the date of publication and the actors and actresses playing in the movie. In that case, the rows would represent one type of movie, and thus another row would represent another movie. This makes the data more structured as each movie is referenced in each row, all the movie titles are categorised under the same column name and the same goes for all the other features. This structuring makes not only easy readable for us, users but also for computers as everything is well structured, and nowadays many programming languages adopted support for reading this type of data format. It is thus possible to access to a specific movie in the tabular data, and one of the example for search for a specific movie is the query search using SQL statement. SQL is a powerful programming language that can retrieve data from a SQL statement. Nonetheless, the Python library Pandas, offers a wide ranges of manipulation tools such as accessing directly for a specific feature of a movie showing only the last movies (in the last rows) in case if the tabular data file is too large. In the case of healthcare domain, hospitals can sometime be faced with thousands of patient records which can be quite time consuming if there are no adapted tools for search for a specific patient.

The second is its versatility in data analysis. In fact, tabular data due to its structure can be easily analysis which makes it suitable for statistical analysis and machine learning techniques. This can be advantageous in fields such as economics or social sciences as there are numerous tools that can perform statistical analysis on tabular data such as the R programming language or Python using the statistical analysis libraries. For example, it is possible to represent an evolution of the cost of a certain product over years or estimate the cost of a wholesale product for a company. In the healthcare domain, analysing patients data can be proven quite useful as they might help on conducting research and studies that help on preventing future diseases or preventing a upcoming disease for a patient. In fact, by analysing the patient disease, it is possible to detect unknown disease or symptoms that can be the overall treatment for all patients faster. Machine learning techniques work very well with this type of data, it is suitable for making for example sales predictions using advanced machine learning models such as decision trees, random forests, and gradient boosting.

Third, the standardization allows the tabular data to have a compatibility support across different software and platforms. Standard formats for the tabular data are CSV (Comma-Separated Values) and Excel spreadsheets are the most well known. The advantage of having the standardisation is that it facilitates data sharing and collaboration among researchers, analysts and organisations. Several converting tools exist, that allow to convert any text format or even Excel spreadsheets format into a CSV format, and vice versa. These tools are even built-in to the software itself, or there are external software that perform such conversion.

Finally, data management is more efficient when using tabular data. Database management systems (DBMS) are using tabular data and they are optimized for storing, querying, and managing large volume of data efficiently. 


\section{Exploring the Challenges of Working with Tabular Data}

% \section{Challenges in Generating High-Quality Synthetic Data}


When it comes to utilising tabular data, the main challenge is the privacy issue. The issue is even more emphasized when it comes to the field of healthcare which every single data is considered as very sensitive. 

\section{Exploring Synthetic Data Generation}

% Navigating Tabular Data Synthesis Research


% LLM-Based Synthetic Datasets: Applications and Limitations in Toxicity Detection

Manipulating with sensitive data is one of the main challenge when it comes to make statistical analysis.
However, synthetic data can help alleviate this issue by generating data that is not representative of the original data, hence the name of synthetic data.

The synthesis of fictional data can also contribute to data augmentation which can be used for downstream improvement of machine learning models as it would serve as training samples for the model. 

% --------------------------------------------------
% Traditional ways of generating synthetic data 
% --------------------------------------------------

\section{Traditional Methods for Synthetic Data Generation}

% synthpop: Bespoke Creation of Synthetic Data in R
% link https://www.jstatsoft.org/article/view/v074i11

% Synthetic Data Generation: A Comparative Study
% link https://dl.acm.org/doi/abs/10.1145/3548785.3548793

% Assessing, visualizing and improving the utility of synthetic data
% link https://arxiv.org/abs/2109.12717

% Providing bespoke synthetic data for the UK Longitudinal Studies and other sensitive data with the synthpop package for R1
% https://content.iospress.com/articles/statistical-journal-of-the-iaos/sji150153

% Simulation of synthetic complex data: The R package simPop
% https://digitalcollection.zhaw.ch/handle/11475/5698


% LLM-Based Synthetic Datasets: Applications and Limitations in Toxicity Detection

Before even considering using LLMs, there are traditional approaches in generating synthetic data. The study LLM-Based Synthetic Datasets: Applications and Limitations in Toxicity Detection shows that mathematical generation (using mathematical operations), synonym replacement (as the name suggests, we simply replace the word by its synonym), and oversampling techniques (which aims to rebalance training data distribution by amplifying the volume of data that is under-represented) can be considered. 


\section{A Look at the Diverse Landscape of Large Language Models}

% LLM-Based Synthetic Datasets: Applications and Limitations in Toxicity Detection

In today's technology, numerous LLMs have surfaced and this has became drastically even more important when OpenAI's ChatGPT 3.5 was made available to public. Since then, other LLMs have started to see the light such as GPT-4, Meta's LLaMa-2 and LLaMa-3 (Large Language Model Meta AI), Mistral from Mistral AI, Gemma and Gemini from Google.


% --------------------------------------------------
% LLM as realistic data generator
% --------------------------------------------------

\section{How Large Language Models are Powering Synthetic Data Generation}

% \section{Large Language Models: A Powerful Tool for Data Synthesis}


% SQBC Active Learning using LLM-Generated Synthetic Data for Stance Detection in Online Political Discussions

The emergence of Large Language Models (LLM) opened new horizons on generating data. It can be used for enhance stance detection in online political discussions as the following study suggests (). Stance detection is an interesting yet complex task in the domain of sentiment analysis that can automatically determine the position (e.g., favor, against, or neutral) of the author. In political discussions, it would help on summarising and detecting misinformation. However, this still remains a very difficult task as the data available to perform the analysis for stance detection is very limited due to privacy reasons. One of the two ways presented in the research article to address this issue is to generate synthetic data by using LLMs, here, authors opted for the quantized (lightweight version) of Mistral v0.1. A prompt is given to the LLM, and the response generated by the model represent the synthetic data that can significantly augment the existing datasets, thus providing more samples to perform stance detection. Results showed substantial improvements in the field of stance detection highlighting the practical benefits of this approach.

% LLM-Based Synthetic Datasets: Applications and Limitations in Toxicity Detection

The study LLM-Based Synthetic Datasets: Applications and Limitations in Toxicity Detection has used a fined-tuned version of GPT-3, called GPT-3 Curie in a context of toxicity detection from online discussions. 
As a matter of fact, such study would help on preventing hate speech, abusive language, and cyberbullying which drawn significant attention due to its legal implications under regulations like the EU's Digital Service Act (DSA). 




% Language Models are realistic tabular data generators











% A Deep Learning Based pipeline for the generation of synthetic tabular data









\section{Prompt engineering is important step in unlocking the potential of large language models}
% \section{Exploring Techniques for Synthetic Data Generation with LLMs}


Prompt engineering is a major step before using LLMs, as they would provide elements of structure and request that model must respect. In case of a bad prompt engineering, this could lead to poor results, bad structure of the response, and a response that could potentially not even correspond to the initial intention of the user. 
If carefully designed, the prompt would help on guiding LLMs towards a satisfactory and relevant response. The study LLM-Based Synthetic Datasets: Applications and Limitations in Toxicity Detection performed prompt engineering by including a general structure to the prompt, a bias mitigation, and evaluation of data variability and coherence. 



% \section{Evaluating the Quality and Effectiveness of Synthetic Data}


\section{Ethical Challenges and Opportunities in Large Language Models}

% \section{Ethical Considerations in LLM-Generated Synthetic Data}


Ethical question is a crucial element when talking about LLMs. As a matter of fact, LLMs, in order to generate a satisfactory response, need to analyse the content of the prompt before generating the results. It is not clearly known how companies who possesses those LLMs actually store the data, but 



% \section{The Future of Synthetic Data with LLMs}
